#!/bin/bash
#SBATCH --job-name=eval_mvimgnet_dino
#SBATCH --output=logs/working_models/eval_mvimgnet_dino_%j.log
#SBATCH --error=logs/working_models/eval_mvimgnet_dino_%j.log
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=8
#SBATCH --time=01:00:00

# ToDo: Remember to change the job name and output file names to match the parameters

echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

module purge
module load 2023
module load Miniconda3/23.5.2-0

eval "$(/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda shell.bash hook)"
conda activate hbird

cd $HOME/open-hummingbird-eval

srun python python_scripts/run_eval.py \
  --model_repo facebookresearch/dino:main \
  --model_name dino_vits16 \
  --batch_size 64 \
  --input_size 512 \
  --patch_size 16 \
  --d_model 768 \
  --nn_method faiss \
  --n_neighbours 30 \
  --augmentation_epoch 1 \
  --dataset_name mvimgnet \
  --data_dir ./datasets/split_angles_mvimagenet \
  --train_bins 90 \
  --val_bins 0 \
  --num_workers 8

# Notes:
# - The model accepts natively input_size=224 (it was trained on this resolution).
#   It will not fail with input_size=512 but it will work suboptimally.
#   To avoid decrease in performance, embeddings should be interpolated.
#   We want to interpolate embeddings and use input_size=512 for consistency.
