#!/bin/bash
#SBATCH --job-name=eval_mvimgnet_dino_v2
#SBATCH --output=logs/working_models/eval_mvimgnet_dino_v2_%j.log
#SBATCH --error=logs/working_models/eval_mvimgnet_dino_v2_%j.log
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=8
#SBATCH --time=00:05:00

# ToDo: Remember to change the job name and output file names to match the parameters

echo "CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES"

module purge
module load 2023
module load Miniconda3/23.5.2-0

eval "$(/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda shell.bash hook)"
conda activate hbird

cd $HOME/open-hummingbird-eval

srun python python_scripts/run_eval.py \
  --model_repo facebookresearch/dinov2:main \
  --model_name dinov2_vits14 \
  --batch_size 64 \
  --input_size 504 \
  --patch_size 14 \
  --d_model 768 \
  --nn_method faiss \
  --n_neighbours 30 \
  --augmentation_epoch 1 \
  --dataset_name mvimgnet \
  --data_dir ./datasets/split_angles_mvimagenet \
  --train_bins 90 \
  --val_bins 0 \
  --num_workers 8

# Notes:
# - Input size should be 504 (model was trained on this resolution).
#   For Dino V2 input_size=224 would also work, but with misaligned embeddings (DINOv2 models allow this silently).
#   This will lead to a drop in performance. To avoid this the embeddings should be interpolated.
#   We want to use input_size=504 for consistency.