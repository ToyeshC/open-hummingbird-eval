#!/bin/bash
#SBATCH --job-name=run_eval_on_pascal_tiny
#SBATCH --output=logs/run_eval_on_pascal_tiny_%j.log
#SBATCH --error=logs/run_eval_on_pascal_tiny_%j.log
#SBATCH --partition=gpu_a100
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gpus=1
#SBATCH --cpus-per-task=16
#SBATCH --time=00:30:00

# ToDo: change the job name and output file names to match the parameters

# Load necessary modules
module purge
module load 2023
module load Miniconda3/23.5.2-0

# Activate the Conda environment (properly)
eval "$(/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda shell.bash hook)"
conda activate hbird

# Navigate to your project directory
cd $HOME/open-hummingbird-eval

# Run the evaluation script
srun python python_scripts/run_eval.py \
  --model_repo facebookresearch/dino:main \
  --model_name dino_vits16 \
  --batch_size 64 \
  --input_size 224 \
  --patch_size 16 \
  --embed_dim 384 \
  --nn_method faiss \
  --n_neighbours 30 \
  --augmentation_epoch 1 \
  --dataset_name voc \
  --data_dir ./datasets/TinyVOCSegmentation \
  --train_fs_path ./datasets/TinyVOCSegmentation/sets/trainaug.txt \
  --val_fs_path ./datasets/TinyVOCSegmentation/sets/val.txt
